Text Segmentation based on Semantic Word Embeddings

Alexander A Alemi
Dept of Physics
Cornell University
aaa244@cornell.edu

Paul Ginsparg
Depts of Physics and Information Science
Cornell University
ginsparg@cornell.edu

ABSTRACT
We explore the use of semantic word embeddings [11, 13,
9] in text segmentation algorithms, including the C99 seg-
mentation algorithm [3, 4] and new algorithms inspired by
the distributed word vector representation. By developing
a general framework for discussing a class of segmentation
ob jectives, we study the eﬀectiveness of greedy versus ex-
act optimization approaches and suggest a new iterative re-
ﬁnement technique for improving the performance of greedy
strategies. We compare our results to known benchmarks
[15, 12, 3, 4], using known metrics [2, 14]. We demonstrate
state-of-the-art performance for an untrained method with
our Content Vector Segmentation (CVS) on the Choi test
set. Finally, we apply the segmentation procedure to an in-
the-wild dataset consisting of text extracted from scholarly
articles in the arXiv.org database.

Categories and Subject Descriptors
I.2.7 [Natural Language Processing]: Text Analysis

General Terms
Information Retrieval, Clustering, Text

Keywords
Text Segmentation, Text Mining, Word Vectors

1.

INTRODUCTION
Segmenting text into naturally coherent sections has many
useful applications in information retrieval and automated
text summarization, and has received much past attention.
An early text segmentation algorithm was the TextTiling
method introduced by Hearst [8] in 1997. Text was scanned
linearly, with a coherence calculated for each adjacent block,
and a heuristic was used to determine the locations of cuts.
In addition to linear approaches, there are text segmenta-
tion algorithms that optimize some scoring ob jective. An

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
KDD ’15 Sydney, Australia
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.

early algorithm in this class was Choi’s C99 algorithm [3]
in 2000, which also introduced a benchmark segmentation
dataset used by subsequent work. Instead of looking only at
nearest neighbor coherence, the C99 algorithm computes a
coherence score between all pairs of elements of text,1 and
searches for a text segmentation that optimizes an ob jective
based on that scoring by greedily making a succession of
best cuts. Later work by Choi and collaborators [4] used dis-
tributed representations of words rather than a bag of words
approach, with the representations generated by LSA [6]. In
2001, Utiyama and Ishahara introduce a statistical model
for segmentation and optimized a posterior for the segment
boundaries. Moving beyond the greedy approaches, in 2004
Fragkou et al. [7] attempted to ﬁnd the optimal splitting
for their own ob jective using dynamic programming. More
recent attempts at segmentation, including Misra et al. [12]
and Riedl and Biemann [15], use LDA based topic models
to inform the segmentation task.
For the most part, the non-topic model based segmenta-
tion approaches have been based on relatively simple rep-
resentations of the underlying text. Recent approaches to
learning word vectors, including Mikolov et al.’s word2vec
[11], Pennington et al.’s GloVe [13] and Levy and Gold-
berg’s pointwise mutual information [9], have proven re-
markably successful in solving analogy tasks, machine trans-
lation [10], and sentiment analysis [13]. These word vector
approaches attempt to learn a log-linear model for word-
word co-occurrence statistics, such that the probability of
two words (w, w (cid:48) ) appearing near one another is propor-
tional to the exponential of their dot product,
(cid:80)
exp(w · w (cid:48) )
P (w|w
v exp(v · w (cid:48) )
The method relies on these word-word co-occurrence statis-
tics encoding meaningful semantic and syntactic relation-
ships. Arora et al. [1] have shown how the remarkable per-
formance of these techniques can be understood in terms of
relatively mild assumptions about corpora statistics, which
in turn can be recreated with a simple generative model.
Here we explore the utility of word vectors for text seg-
mentation, both in the context of existing algorithms such
as C99, and when used to construct new segmentation ob-
jectives based on a generative model for segment formation.
We will ﬁrst construct a framework for describing a family
of segmentation algorithms, then discuss the speciﬁc algo-

) =

(cid:48)

.

(1)

1By ‘elements’, we mean the pieces of text combined in order
to comprise the segments. In the applications to be consid-
ered, the basic elements will be either sentences or words.

rithms to be investigated in detail. We then apply our mod-
iﬁed algorithms both to the standard Choi test set and to a
test set generated from arXiv.org research articles.
2. TEXT SEGMENTATION
The segmentation task is to split a text into contiguous
coherent sections. We ﬁrst build a representation of the text,
by splitting it into N basic elements, (cid:126)Vi (i = 1, . . . , N ), each
a D-dimensional feature vector Viα (α = 1, . . . , D) repre-
senting the element. Then we assign a score σ(i, j ) to each
candidate segment, comprised of the ith through (j − 1)th
elements, and ﬁnally determine how to split the text into
the appropriate number of segments.
Denote a segmentation of text into K segments as a list
of K indices s = (s1 , s1 , · · · , sK ), where the k-th segment
includes the elements (cid:126)Vi with sk−1 ≤ i < sk , with s0 ≡
0. For example, the string “aaabbcccdd” considered at the
character level would be properly split with s = (3, 5, 8, 10)
into (“aaa”, “bb”, “ccc”, “dd”).
2.1 Representation
The text representation thus amounts to turning a plain
text document T into an (N × D)-dimensional matrix V,
with N the number of initial elements to be grouped into
coherent segments and D the dimensionality of the element
representation. For example, if segmenting at the word level
then N would be the number of words in the text, and each
word might be represented by a D-dimensional vector, such
as those obtained from GloVe [13]. If segmenting instead at
the sentence level, then N is the number of sentences in the
text and we must decide how to represent each sentence.
There are additional preprocessing decisions, for example
using a stemming algorithm or removing stop words before
forming the representation. Particular preprocessing deci-
sions can have a large eﬀect on the performance of segmen-
tation algorithms, but for discussing scoring functions and
splitting methods those decisions can be abstracted into the
speciﬁcation of the N × D matrix V.
2.2 Scoring
Having built an initial representation the text, we next
specify the coherence of a segment of text with a scoring
function σ(i, j ), which acts on the representation V and re-
turns a score for the segment running from i (inclusive) to
j (non-inclusive). The score can be a simple scalar or more
general ob ject. In addition to the scoring function, we need
to specify how to return an aggregrate score for the entire
segmentation. This score aggregation function ⊕ can be as
simple as adding the scores for the individual segments, or
again some more general function. The score S (s) for an
overall segmentation is given by aggregating the scores of
all of the segments in the segmentation:
S (s) = σ(0, s1 ) ⊕ σ(s1 , s2 ) ⊕ · · · ⊕ σ(sK−1 , sK ) .
(2)
Finally, to frame the segmentation problem as a form of
gle scalar. The key function ((cid:74)·(cid:75)) returns this single number,
optimization, we need to map the aggregated score to a sin-
C (s) = (cid:74)S (s)(cid:75) .
so that the cost for the above segmentation is
(3)
For most of the segmentation schemes to be considered,
the score function itself returns a scalar, so the score aggre-
gation function ⊕ will be taken as simple addition with the

key function the identity, but the generality here allows us
to incorporate the C99 segmentation algorithm [3] into the
same framework.
2.3 Splitting
Having speciﬁed the representation of the text and scor-
ing of the candidate segments, we need to prescribe how to
choose the ﬁnal segmentation.
In this work, we consider
three methods: (1) greedy splitting, which at each step in-
serts the best available segmentation boundary; (2) dynamic
programming based segmentation, which uses dynamic pro-
gramming to ﬁnd the optimal segmentation; and (3) an it-
erative reﬁnement scheme, which starts with the greedy seg-
mentation and then adjusts the boundaries to improve per-
formance.
2.3.1 Greedy Segmentation
The greedy segmentation approach builds up a segmenta-
tion into K segments by greedily inserting new boundaries
at each step to minimize the aggregate score:
s0 = {N }
st+1 = arg min
i∈[1,N )

C (st ∪ {i})

(4)

(5)

until the desired number of splits is reached. Many published
text segmentation algorithms are greedy in nature, including
the original C99 algorithm [3].
2.3.2 Dynamic Programming
The greedy segmentation algorithm is not guaranteed to
ﬁnd the optimal splitting, but dynamic programming meth-
ods can be used for the text segmentation problem formu-
lated in terms of optimizing a scoring ob jective. For a de-
tailed account of dynamic programming and segmentation
in general, see the thesis by Terzi [16]. Dynamic program-
ming as been applied to text segmentation in Fragkou et
al. [7], with much success, but we will also consider here an
optimizaton of the the C99 segmentation algorithm using a
dynamic programming approach.
The goal of the dynamic programming approach is to split
the segmentation problem into a series of smaller segmenta-
tion problems, by expressing the optimal segmentation of the
ﬁrst n elements of the sequence into k segments in terms of
the best choice for the last segmentation boundary. The ag-
be minimized with respect to the key function (cid:74)·(cid:75):
gregated score S (n, k) for this optimal segmentation should
S (n, 1) = σ(0, n)
S (n, k) = (cid:74)·(cid:75)min
S (l, k − 1) ⊕ σ(l, n) .
l<n
While the dynamic programming approach yeilds the op-
timal segmentation for our decomposable score function, it
can be costly to compute, especially for long texts. In prac-
tice, both the optimal segmentation score and the resulting
segmentation can be found in one pass by building up a ta-
ble of segmentation scores and optimal cut indices one row
at a time.
2.3.3
Iterative Relaxation
Inspired by the popular Lloyd algorithm for k-means, we
attempt to retain the computational beneﬁt of the greedy
segmentation approach, but realize additional performance
gains by iteratively reﬁning the segmentation. Since text

(7)

(6)

st+1
k =

segmentation problems require contiguous blocks of text, a
natural scheme for relaxation is to try to move each segment
boundary optimally while keeping the edges to either side of
(cid:0)σ(0, st
it ﬁxed:
(cid:74)·(cid:75)
1 ) ⊕ · · ·
K )(cid:1) (8)
arg min
l∈(st
k−1 , st
k+1 )
S (cid:0)st − {st
k } ∪ {l}(cid:1)
k+1 ) ⊕ · · · ⊕ σ(st
k−1 , l) ⊕ σ(l, st
⊕ σ(st
K−1 , st
(cid:74)·(cid:75)
(9)
arg min
l∈(st
k−1 , st
k+1 )
We will see in practice that by 20 iterations it has typically
converged to a ﬁxed point very close to the optimal dynamic
programming segmentation.

=

(10)

Aij =

3. SCORING FUNCTIONS
In the experiments to follow, we will test various choices
for the representation, scoring function, and splitting method
in the above general framework. The segmentation algo-
rithms to be considered fall into three groups:
3.1 C99 Segmentation
Choi’s C99 algorithm [3] was an early text segmentation
algorithm with promising results. The feature vector for an
element of text is chosen as the pairwise cosine distances
with other elements of text, where those elements in turn
(cid:80)
are represented by a bag of stemmed words vector (after
preprocessing to remove stop words):
(cid:113)(cid:80)
(cid:80)
w fi,w fj,w
w f 2
w f 2
i,w
j,w
with fi,w the frequency of word w in element i. The pair-
wise cosine distance matrix is noisy for these features, and
since only the relative values are meaningful, C99 employs a
ranking transformation, replacing each value of the matrix
(cid:88)
(cid:88)
by the fraction of its neighbors with smaller value:
i−r/2≤l≤i+r/2
j−r/2≤m≤j+r/2
l(cid:54)=i
m (cid:54)=j
(11)
where the neighborhood is an r × r block around the en-
try, the square brackets mean 1 if the inequality is satisﬁed
otherwise 0 (and values oﬀ the end of the matrix are not
counted in the sum, or towards the normalization). Each
element of the text in the C99 algorithm is represented by a
rank transformed vector of its cosine distances to each other
element.
(cid:80)
The score function describes the average intersentence sim-
ilarity by taking the overall score to be
k βk(cid:80)
where βk = (cid:80)
(cid:80)
C (s) =
k αk
Vij is the sum of all
sk−1≤j<sk
sk−1≤i<sk
ranked cosine similarities in a segment and αk = (sk+1 −sk )2
is the squared length of the segment. This score function
Vij , (j − i)2(cid:17)
(cid:16) (cid:88)
is still decomposable, but requires that we deﬁne the local
(cid:88)
score function to return a pair,
i≤k<j
i≤k<j

[Aij > Alm ] ,

1
r2 − 1

σ(i, j ) =

Vij =

(12)

(13)

,

,

,

with score aggregation function deﬁned as component addi-
tion,

.

β
α

(15)

(14)

(β1 , α1 ) ⊕ (β2 , α2 ) = (β1 + β2 , α1 + α2 ) ,
and key function deﬁned as division of the two components,
(cid:74)(β , α)(cid:75) =
While earlier work with the C99 algorithm considered only
a greedy splitting approach, in the experiments that follow
we will use our more general framework to explore both op-
timal dynamic programming and reﬁned iterative versions
of C99. Followup work by Choi et al. [4] explored the eﬀect
of using combinations of LSA word vectors in eq. (10) in
place of the fi,w . Below we will explore the eﬀect of using
combinations of word vectors to represent the elements.
3.2 Average word vector
To assess the utility of word vectors in segmentation, we
ﬁrst investigate how they can be used to improve the C99
algorithm, and then consider more general scoring functions
based on our word vector representation. As the represen-
(cid:88)
tation of an element, we take
w

fiw vwk ,

Vik =

(16)

with fiw representing the frequency of word w in element i,
and vwk representing the kth component of the word vector
for word w as learned by a word vector training algorithm,
such as word2vec [11] or GloVe [13].
The length of word vectors varies strongly across the vo-
cabulary and in general correlates with word frequency. In
order to mitigate the eﬀect of common words, we will some-
times weight the sum by the inverse document frequency
(cid:88)
(idf ) of the word in the corpus:
w

|D|
dfw

fiw log

Vik =

vwk ,

(17)

where dfw is the number of documents in which word w
appears. We can instead normalize the word vectors before
(cid:88)
vwk(cid:112)(cid:80)
adding them together
k v2
wk
w

fiw ˜vwk

˜vwk =

Vik =

(18)

,

or both weight by idf and normalize.
Segmentation is a form of clustering, so a natural choice
for scoring function is the sum of square deviations from the
(cid:88)
(cid:88)
(cid:0)Vlk − µk (i, j )(cid:1)2
mean of the segment, as used in k-means:
j−1(cid:88)
k
l
1
j − i
l=i

where µk (i, j ) =

σ(i, j ) =

Vlk ,

(19)

(20)

and which we call the Euclidean score function. Generally,
however, cosine similarity is used for word vectors, making
angles between words more important than distances.
In
some experiments, we therefore normalize the word vectors
ﬁrst, so that a euclidean distance score better approximates
the cosine distance (recall |˜v − ˜w|2
2 = |˜v |2
2 + | ˜w|2
2 − 2˜v · ˜w =
2(1 − ˜v · ˜w) for normalized vectors).

(21)

3.3 Content Vector Segmentation (CVS)
Trained word vectors have a remarkable amount of struc-
ture. Analogy tasks such as man:woman::king:? can be
solved by ﬁnding the vector closest to the linear query:
vwoman − vman + vking .
Arora et al. [1] constructed a generative model of text that
explains how this linear structure arises and can be main-
tained even in relatively low dimensional vector models. The
generative model consists of a content vector which under-
goes a random walk from a stationary distribution deﬁned
to be the product distribution on each of its components ck ,
uniform on the interval [− 1√
, 1√
] (with D the dimension-
D
D
ality of the word vectors). At each point in time, a word
(cid:88)
vector is generated by the content vector according to a log-
linear model:
P (w|c) =
v

exp(w · c) , Zc =

exp(v · c) .

1
Zc

(22)

The slow drift of the content vectors helps to ensure that
nearby words obey with high probability a log-linear model
for their co-occurence probability:
(cid:107)vw + vw(cid:48) (cid:107)2 − 2 log Z ± o(1) ,

log P (w, w

(23)

) =

(cid:48)

1
2d

(24)

for some ﬁxed Z .
To segment text into coherent sections, we will boldly as-
sume that the content vector in each putative segment is
constant, and measure the log likelihood that all words in the
segment are drawn from the same content vector c. (This is
similar in spirit to the probabilistic segmentation technique
(cid:88)
log P (wi |c) ∝ (cid:88)
proposed by Utiyama and Isahara [17].) Assuming the word
draws {wi } are independent, we have that the log likelihood
wi · c
log P ({wi }|c) =
i
i
is proportional to the sum of the dot products of the word
vectors wi with the content vector c. We use a maximum
likelihood estimate for the content vector:
(cid:16)
(cid:17)
log P (c|{wi })
c = arg max
c
log P ({wi }|c) + log P (c) − log P ({wi })
(cid:88)
= arg max
c
s.t. − 1√
1√
∝ arg max
wi · c
D
D
c
(cid:88)
(cid:88)
This determines what we will call the Content Vector Seg-
mentation (CVS) algorithm, based on the score function
i≤l<j
k

wlk ck (i, j ) .

σ(i, j ) =

< ck <

(26)

(25)

(27)

(28)

.

The score σ(i, j ) for a segment (i, j ) is the sum of the dot
products of the word vectors wlk with the maximum likeli-
 1√
 (cid:88)
hood content vector c(i, j ) for the segment, with components
given by
D
i≤l<j
The maximum likelihood content vector thus has compo-
nents ± 1√
, depending on whether the sum of the word
D
vector components in the segment is positive or negative.

ck (i, j ) = sign

(29)

wl,k

.

This score function will turn out to generate some of the
most accurate segmentation results. Note that CVS is com-
pletely untrained with respect to the speciﬁc text to be seg-
mented, relying only on a suitable set of word vectors, de-
rived from some corpus in the language of choice. While
CVS is most justiﬁable when working on the word vectors
directly, we will also explore the eﬀect of normalizing the
word vectors before applying the ob jective.

4. EXPERIMENTS
To explore the eﬃcacy of diﬀerent segmentation strategies
and algorithms, we performed segmentation experiments on
two datasets. The ﬁrst is the Choi dataset [3], a common
benchmark used in earlier segmentation work, and the sec-
ond is a similarly constructed dataset based on articles up-
loaded to the arXiv, as will be described in Section 4.3. All
code and data used for these experiments is available on-
line2 .
4.1 Evaluation
To evaluate the performance of our algorithms, we use
two standard metrics: the Pk metric and the WindowDiﬀ
(WD) metric. For text segmentation, near misses should
get more credit than far misses. The Pk metric [2], captures
the probability for a probe composed of a pair of nearby ele-
ments (at constant distance positions (i, i + k)) to be placed
in the same segment by both reference and hypothesized seg-
mentations. In particular, the Pk metric counts the number
N −k(cid:88)
(cid:2)δhyp (i, i + k) (cid:54)= δref (i, i + k)(cid:3)
of disagreements on the probe elements:
i=1
− 1 ,
# elements
1
k =
2
# segments
nearest
integer

1
N − k

Pk =

(30)

where δ(i, j ) is equal to 1 or 0 according to whether or not
both element i and j are in the same segment in hypothe-
sized and reference segmentations, resp., and the argument
of the sum tests agreement of the hypothesis and reference
segmentations. (k is taken to be one less than the integer
closest to half of the number of elements divided by the
number of segments in the reference segmentation.) The
total is then divided by the total number of probes. This
metric counts the number of disagreements, so lower scores
indicate better agreement between the two segmentations.
Trivial strategies such as choosing only a single segmen-
tation, or giving each element its own segment, or giving
constant boundaries or random boundaries, tend to produce
values of around 50% [2].
The Pk metric has the disadvantage that it penalizes false
positives more severely than false negatives, and can suﬀer
when the distribution of segment sizes varies. Pevzner and
N −k(cid:88)
(cid:2)bref (i, i + k) (cid:54)= bhyp (i, i + k)(cid:3) ,
Hearst [14] introduced the WindowDiﬀ (WD) metric:
i=1
where b(i, j ) counts the number of boundaries between loca-
tion i and j in the text, and an error is registered if the hy-
pothesis and reference segmentations disagree on the num-
ber of boundaries. In practice, the Pk and WD scores are
2 github.com/alexalemi/segmentation

1
N − k

W D =

(31)

highly correlated, with Pk more prevalent in the literature
— we will provide both for most of the experiments here.
4.2 Choi Dataset
The Choi dataset is used to test whether a segmentation
algorithm can distinguish natural topic boundaries. It con-
catenates the ﬁrst n sentences from ten diﬀerent documents
chosen at random from the Brown corpus. The number of
sentences taken from each document is chosen uniformly at
random within a range speciﬁed by the subset id (in the form
minimum–maximum #sentences). There are four ranges
considered: (3–5, 6–8, 9–11, 3–11), the ﬁrst three of which
have 100 example documents, and the last 400 documents.
The dataset can be obtained from an archived version of the
C99 segmentation code release3 . An extract from one of the
documents in the test set is shown in Fig. 1.

1
2

3

4

5

6
7

8
9

10
11

12

= = = = = = = = = =
S o m e o f t h e f e a t u r e s o f t h e t o p p o r t i o n s o f F i g u r e 1
a n d F i g u r e 2 w e r e m e n t i o n e d i n d i s c u s s i n g T a b l e 1
.
F i r s t , t h e O n s e t P r o f i l e s p r e a d s a c r o s s
a p p r o x i m a t e l y 1 2 y e a r s f o r b o y s a n d 1 0 y e a r s f o r
g i r l s .
I n c o n t r a s t , 2 0 o f t h e 2 1 l i n e s i n t h e C o m p l e t i o n
P r o f i l e ( e x c l u d i n g c e n t e r 5 f o r b o y s a n d 4 f o r
g i r l s ) a r e b u n c h e d a n d e x t e n d o v e r a m u c h
s h o r t e r p e r i o d , a p p r o x i m a t e l y 3 0 m o n t h s f o r b o y s
a n d 4 0 m o n t h s f o r g i r l s .
T h e M a t u r i t y C h a r t f o r e a c h s e x d e m o n s t r a t e s c l e a r l y
t h a t O n s e t i s a p h e n o m e n o n o f i n f a n c y a n d e a r l y
c h i l d h o o d w h e r e a s C o m p l e t i o n i s a p h e n o m e n o n o f
t h e l a t e r p o r t i o n o f a d o l e s c e n c e .
= = = = = = = = = =
T h e m a n y l i n g u i s t i c t e c h n i q u e s f o r r e d u c i n g t h e
a m o u n t o f d i c t i o n a r y i n f o r m a t i o n t h a t h a v e b e e n
p r o p o s e d a l l o r g a n i z e t h e d i c t i o n a r y ’ s c o n t e n t s
a r o u n d p r e f i x e s , s t e m s , s u f f i x e s , e t c .

.
A s i g n i f i c a n t r e d u c t i o n i n t h e v o u m e o f s t o r e
i n f o r m a t i o n i s t h u s r e a l i z e d , e s p e c i a l l y f o r a
h i g h l y i n f l e c t e d l a n g u a g e s u c h a s R u s s i a n .
F o r E n g l i s h t h e r e d u c t i o n i n s i z e i s l e s s s t r i k i n g .
T h i s a p p r o a c h r e q u i r e s t h a t : ( 1 ) e a c h t e x t w o r d b e
s e p a r a t e d i n t o s m a l l e r e l e m e n t s t o e s t a b l i s h a
c o r r e s p o n d e n c e b e t w e e n t h e o c c u r r e n c e a n d
d i c t i o n a r y e n t r i e s , a n d ( 2 ) t h e i n f o r m a t i o n
r e t r i e v e d f r o m s e v e r a l e n t r i e s i n t h e d i c t i o n a r y
b e s y n t h e s i z e d i n t o a d e s c r i p t i o n o f t h e
p a r t i c u l a r w o r d .
= = = = = = = = = =

Figure 1: Example of two segments from the Choi
dataset, taken from an entry in the 3–5 set. Note the
appearance of a “sentence” with the single character
“.” in the second segment on line 8. These short
sentences can confound the benchmarks.

4.2.1 C99 benchmark
We will explore the eﬀect of changing the representation
and splitting strategy of the C99 algorithm.
In order to
give fair comparisons we implemented our own version of
the C99 algorithm (oC99). The C99 performance depended
sensitively on the details of the text preprocessing. Details
can be found in Appendix A.

3 http://web.archive.org/web/20010422042459/http://
www.cs.man.ac.uk/~choif/software/C99- 1.2- release.
tgz (We thank with Martin Riedl for pointing us to the
dataset.)

4.2.2 Effect of word vectors on C99 variant
The ﬁrst experiment explores the ability of word vectors
to improve the performance of the C99 algorithm. The word
vectors were learned by GloVe [13] on a 42 billion word set of
the Common Crawl corpus in 300 dimensions4 . We empha-
size that these word vectors were not trained on the Brown
or Choi datasets directly, and instead come from a general
corpus of English. These vectors were chosen in order to
isolate any improvement due to the word vectors from any
confounding eﬀects due to details of the training procedure.
The results are summarized in Table 1 below. The upper
section cites results from [4], exploring the utility of using
LSA word vectors, and showed an improvement of a few
percent over their baseline C99 implementation. The mid-
dle section shows results from [15], which augmented the
C99 method by representing each element with a histogram
of topics learned from LDA. Our results are in the lower
section, showing how word vectors improve the performance
of the algorithm.
In each of these last experiments, we turned oﬀ the rank
transformation, pruned the stop words and punctuation, but
did not stem the vocabulary. Word vectors can be incorpo-
rated in a few natural ways. Vectors for each word in a
sentence can simply be summed, giving results shown in the
oC99tf row. But all words are not created equal, so the sen-
tence representation might be dominated by the vectors for
common words. In the oC99tﬁdf row, the word vectors are
weighted by idfi = log 500
(i.e., the log of the inverse doc-
dfi
ument frequency of each word in the Brown corpus, which
has 500 documents in total) before summation. We see some
improvement from using word vectors, for example the Pk
of 14.78% for the oC99tﬁdf method on the 3–11 set, com-
pared to Pk of 15.56% for our baseline C99 implementation.
On the shorter 3–5 test set, our oC99tﬁdf method achieves
Pk of 10.27% versus the baseline oC99 Pk of 14.22% . To
compare to the various topic model based approaches, e.g.
[15], we perform spherical k-means clustering on the word
vectors [5] and represent each sentence as a histogram of
its word clusters (i.e., as a vector in the space of clusters,
with components equal to the number of its words in that
that cluster). In this case, the word topic representations
(oC99k50 and oC99k200 in Table 1) do not perform as well
as the C99 variants of [15]. But as was noted in [15], those
topic models were trained on cross-validated subsets of the
Choi dataset, and beneﬁted from seeing virtually all of the
sentences in the test sets already in each training set, so
have an unfair advantage that would not necessarily convey
to real world applications. Overall, the results in Table 1
illustrate that the word vectors obtained from GloVe can
markedly improve existing segmentation algorithms.

4.2.3 Alternative Scoring frameworks
The use of word vectors permits consideration of natural
scoring functions other than C99-style segmentation scoring.
The second experiment examines alternative scoring frame-
works using the same GloVe word vectors as in the previous
experiment. To test the utility of the scoring functions more
directly, for these experiments we used the optimal dynamic
programming segmentation. Results are summarized in Ta-
ble 2, which shows the average Pk and WD scores on the

4Obtainable from http://www- nlp.stanford.edu/data/
glove.42B.300d.txt.gz

Algorithm
C99 [4]
C99LSA
C99 [15]
C99LDA
oC99
oC99tf
oC99tﬁdf
oC99k50
oC99k200

3–5
12
9

14.22
12.14
10.27
20.39
18.60

Pk

9–11
9
7

6–8
11
10
11.20
4.16
11.59
12.20
13.17
14.60
15.87
12.23
23.76
21.13
17.37
19.42

3–5

WD
6–8

9–11

3–11

3–11
9
5

12.07
4.89
11.60
12.22
13.34
15.22
16.29
12.30
23.26
21.34
17.42
19.60

15.64
15.22
14.96
24.63
20.97

15.56
14.91
14.78
24.33
20.85

14.22
12.14
10.27
20.39
18.60

Table 1: Eﬀect of using word vectors in the C99 text segmentation algorithm. Pk and WD results are shown
(smaller values indicate better performance). The top section (C99 vs. C99LSA) shows the few percent
improvement over the C99 baseline reported in [4] of using LSA to encode the words. The middle section
(C99 vs. C99LDA) shows the eﬀect of modifying the C99 algorithm to work on histograms of LDA topics in
each sentence, from [15]. The bottom section shows the eﬀect of using word vectors trained from GloVe [13]
in our oC99 implementation of the C99 segmentation algorithm. The oC99tf implementation sums the word
vectors in each sentence, with no rank transformation, after removing stop words and punctuation. oC99tﬁdf
weights the sum by the log of the inverse document frequency of each word. The oC99k models use the word
vectors to form a topic model by doing spherical k-means on the word vectors. oC99k50 uses 50 clusters and
oC99k200 uses 200.

Algorithm rep
tf
tﬁdf

oC99

Euclidean

Content (CVS)

tf

tﬁdf

tf

tﬁdf

Pk WD
n
11.94
11.78
-
12.19
12.27
-
8.28
7.68
F
10.83
T
9.18
14.27
F 12.89
8.95
8.32
T
F
5.29
5.39
5.55
5.42
T
5.87
5.75
F
T
5.03
5.12

Table 2: Results obtained by varying the scoring
function. These runs were on the 3–11 set from the
Choi database, with a word cut of 5 applied, after
preprocessing to remove stop words and punctua-
tion, but without stemming. The CVS method does
remarkably better than either the C99 method or a
Euclidean distance-based scoring function.

3–11 subset of the Choi dataset. In all cases, we removed
stop words and punctuation, did not stem, but after prepro-
cessing removed sentences with fewer than 5 words.
Note ﬁrst that the dynamic programming results for our
implementation of C99 with tf weights gives Pk = 11.78%,
3% better than the greedy version result of 14.91% reported
in Table 1. This demonstrates that the original C99 algo-
rithm and its applications can beneﬁt from a more exact
minimization than given by the greedy approach. We con-
sidered two natural score functions: the Euclidean scoring
function (eqn. (20)) which minimizes the sum of the square
deviations of each vector in a segment from the average vec-
tor of the segment, and the Content Vector scoring (CVS)
(eqn. (28) of section 3.3), which uses an approximate log
posterior for the words in the segment, as determined from
its maximum likelihood content vector.
In each case, we
consider vectors for each sentence generated both as a strict
sum of the words comprising it (tf approach), and as a sum

weighted by the log idf (tﬁdf approach, as in sec. 4.2.2). Ad-
ditionally, we consider the eﬀect of normalizing the element
vectors before starting the score minimization, as indicated
by the n column.
The CVS score function eqn. (28) performs the best over-
all, with Pk scores below 6%, indicating an improved seg-
mentation performance using a score function adapted to
the choice of representation. While the most principled
score function would be the Content score function using
tf weighted element vectors without normalization, the nor-
malized tﬁdf scheme actually performs the best. This is
probably due to the uncharacteristically large eﬀect com-
mon words have on the element representation, which the
log idf weights and the normalization help to mitigate.
Strictly speaking, the idf weighted schemes cannot claim
to be completely untrained, as they beneﬁt from word usage
statistics in the Choi test set, but the raw CVS method still
demonstrates a marked improvement on the 3–11 subset,
5.29% Pk versus the optimal C99 baseline of 11.78% Pk .
4.2.4 Effect of Splitting Strategy
To explore the eﬀect of the splitting strategy and to com-
pare with our overall results on the Choi test set against
other published benchmarks, in our third experiment we ran
the raw CVS method against all of the Choi test subsets, us-
ing all three splitting strategies discussed: greedy, reﬁned,
and dynamic programming. These results are summarized
in Table 3.
Overall, our method outperforms all previous untrained
methods. As commented regarding Table 1 (toward the end
of subsection 4.2.2), we have included the results of the topic
modelling based approaches M09 [12] and R12 [15] for ref-
erence. But due to repeat appearance of the same sentences
throughout each section of the Choi dataset, methods that
split that dataset into test and training sets have unavoid-
able access to the entirety of the test set during training, al-
beit in diﬀerent order.5 These results can therefore only be

5 In [15], it is observed that “This makes the Choi data set
artiﬁcially easy for supervised approaches.”

Alg
TT [3]
C99 [3]
C01 [4]
U00 [17]
F04 [7]
G-CVS
R-CVS
DP-CVS
M09 [12]
R12 [15]

3–5
44
12
10
9
5.5
5.14
3.92
3.41
2.2
1.24

6–8
43
9
7
7
3.0
4.82
3.75
3.45
2.3
0.76

9–11
48
9
5
5
1.3
6.38
5.17
4.45
4.1
0.56

3–11
46
12
9
10
7.0
6.49
5.65
5..29
2.3
0.95

Table 3: Some published Pk results on the Choi
dataset against our raw CVS method. G-CVS uses
a greedy splitting strategy, R-CVS uses up to 20 it-
erations to reﬁne the results of the greedy strategy,
and DP-CVS shows the optimal results obtained by
dynamic programming. We include the topic mod-
eling results M09 and R12 for reference, but for rea-
sons detailed in the text do not regard them as com-
parable, due to their mingling of test and training
samples.

Figure 2: Results from last column of Table 3 repro-
duced to highlight the performance of the CVS seg-
mentation algorithm compared to similar untrained
algorithms. Its superior performance in an unsuper-
vised setting suggests applications on documents “in
the wild”.

compared to other algorithms permitted to make extensive
use of the test data during cross-validation training. Only
the TT, C99, U00 and raw CVS method can be considered
as completely untrained. The C01 method derives its LSA
vectors from the Brown corpus, from which the Choi test set
is constructed, but that provides only a weak beneﬁt, and
the F04 method is additionally trained on a subset of the
test set to achieve its best performance, but its use only of
idf values provides a similarly weak beneﬁt.
We emphasize that the raw CVS method is completely
independent of the Choi test set, using word vectors derived
from a completely diﬀerent corpus.
In Fig. 2, we repro-
duce the relevant results from the last column of Table 1 to
highlight the performance beneﬁts provided by the semantic
word embedding.
Note also the surprising performance of the reﬁned split-
ting strategy, with the R-CVS results in Table 3 much lower
than the greedy G-CVS results, and moving close to the op-
timal DP-CVS results, at far lower computational cost. In
particular, taking the dynamic programming segmentation
as the true segmentation, we can assess the performance of

R-CVS vs DP-CVS [3]

3–5
0.90

6–8
0.65

9–11
1.16

3–11
1.15

Table 4: Treating the dynamic programming splits
as the true answer, the error of the reﬁned splits as
measured in Pk across the subsets of the Choi test
set.

the reﬁned strategy. As seen in Table 4, the reﬁned segmen-
tation very closely approximates the optimal segmentation.
This is important in practice since the dynamic program-
ming segmentation is much slower, taking ﬁve times longer
to compute on the 3–11 subset of the Choi test set. The
dynamic programming segmentation becomes computation-
ally infeasible to do at the scale of word level segmentation
on the arXiv dataset considered in the next section, whereas
the reﬁned segmentation method remains eminently feasible.
4.3 ArXiv Dataset
Performance evaluation on the Choi test set implements
segmentation at the sentence level, i.e., with segments of
composed of sentences as the basic elements. But text sources
do not necessarily have well-marked sentence boundaries.
The arXiv is a repository of scientiﬁc articles which for prac-
tical reasons extracts text from PDF documents (typically
using pdfminer/pdf2txt.py). That Postscript-based for-
mat was originally intended only as a means of formatting
text on a page, rather than as a network transmission for-
mat encoding syntactic or semantic information. The result
is often somewhat corrupted, either due to the handling of
mathematical notation, the presence of footers and headers,
or even just font encoding issues.
To test the segmentation algorithms in a realistic set-
ting, we created a test set similar to the Choi test set, but
based on text extracted from PDFs retrieved from the arXiv
database. Each test document is composed of a random
number of contiguous words, uniformly chosen between 100
and 300, sampled at random from the text obtained from
arXiv articles. The text was preprocessed by lowercasing
and inserting spaces around every non-alphanumeric char-
acter, then splitting on whitespace to tokenize. An example
of two of the segments of the ﬁrst test document is shown
in Figure 3 below.
This is a much more diﬃcult segmentation task: due to
the presence of numbers and many periods in references,
there are no clear sentence boundaries on which to initially
group the text, and no natural boundaries are suggested in
the test set examples. Here segmentation algorithms must
work directly at the “word” level, where word can mean a
punctuation mark. The presence of garbled mathematical
formulae adds to the diﬃculty of making sense of certain
streams of text.
In Table 5, we summarize the results of three word vector
powered approaches, comparing a C99 style algorithm to
our content vector based methods, both for unnormalized
and normalized word vectors. Since much of the language
of the scientiﬁc articles is specialized, the word vectors used
in this case were obtained from GloVe trained on a corpus
of similarly preprocessed texts from 98,392 arXiv articles.
(Since the elements are now words rather than sentences,
the only issue involves whether or not those word vectors
are normalized.) As mentioned, the dynamic programming
approach is prohibitively expensive for this dataset.

024681012PkDP-CVSR-CVSG-CVSF04U00C01C991

2

. n a t u r e _ 4 1 4 : 4 4 1 - 4 4 3 . 1 2 s e i n e n , i . a n d s c h r a m
a . 2 0 0 6 . s o c i a l _ s t a t u s a n d g r o u p n o r m s :
i n d i r e c t _ r e c i p r o c i t y i n a h e l p i n g e x p e r i m e n t .
e u r o p e a n _ e c o n o m i c _ r e v i e w 5 0 : 5 8 1 - 6 0 2 . s i l v a ,
e . r . , j a f f e , k . 2 0 0 2 . e x p a n d e d f o o d
c h o i c e a s a p o s s i b l e f a c t o r i n t h e e v o l u t i o n o f
e u s o c i a l i t y i n v e s p i d a e s o c i o b i o l o g y 3 9 : 2 5 - 3 6
. s m i t h , j . , v a n d y k e n , j . d . , z e e j u n e ,
p . c . 2 0 1 0 . a g e n e r a l i z a t i o n o f h a m i l t o n ’ s
r u l e f o r t h e e v o l u t i o n o f m i c r o b i a l c o o p e r a t i o n
s c i e n c e _ 3 2 8 , 1 7 0 0 - 1 7 0 3 . z h a n g , j . , w a n g ,
j . , s u n , s . , w a n g , l . , w a n g , z . , x i a ,
c . 2 0 1 2 . e f f e c t o f g r o w i n g s i z e o f i n t e r a c t i o n
n e i g h b o r s o n t h e e v o l u t i o n o f c o o p e r a t i o n i n
s p a t i a l s n o w d r i f t _ g a m e . c h i n e s e _ s c i e n c e b u l l e t i n
5 7 : 7 2 4 - 7 2 8 . z i m m e r m a n , m . , e g u ‘ i l u z ,
v . , s a n _ m i g u e l ,
o f ) e , e q u i p p e d _ w i t h t h e t o p o l o g y o f
w e a k _ c o n v e r g e n c e . w e w i l l s t a t e s o m e r e s u l t s
a b o u t r a n d o m m e a s u r e s . 1 0 d e f i n i t i o n a . 1 (
f i r s t t w o m o m e n t m e a s u r e s ) . f o r a
r a n d o m _ v a r i a b l e z , t a k i n g v a l u e s i n p ( e ) ,
a n d k = 1 , 2 , . . . , t h e r e i s a
u n i q u e l y _ d e t e r m i n e d m e a s u r e µ ( k ) o n b ( e k )
s u c h t h a t e [ z ( a 1 ) ·_ ·_ · z ( a k ) ] = µ ( k )
( a 1 × · _ ·_ · × a k ) f o r a 1 , . . . , a k ∈ b ( e )
. t h i s i s c a l l e d t h e k t h _ m o m e n t m e a s u r e .
e q u i v a l e n t l y , µ ( k ) i s t h e u n i q u e m e a s u r e s u c h
t h a t e [ h z , φ 1 i · _ · _ · h z , φ k i ] = h µ ( k )
, φ 1 ·_ ·_ · φ k i , w h e r e h . , . i d e n o t e s
i n t e g r a t i o n . l e m m a a . 2 ( c h a r a c t e r i s a t i o n o f
d e t e r m i n i s t i c r a n d o m m e a s u r e s ) . l e t z b e a
r a n d o m _ v a r i a b l e _ t a k i n g v a l u e s i n p ( e ) w i t h t h e
f i r s t t w o m o m e n t m e a s u r e s µ : = µ ( 1 ) a n d µ (
2 ) . t h e n t h e
f o l l o w i n g _ a s s e r t i o n s _ a r e _ e q u i v a l e n t : 1 . t h e r e
i s ν ∈ p ( e ) w i t h z = ν , a l m o s t _ s u r e l y . 2 .
t h e s e c o n d _ m o m e n t m e a s u r e h a s p r o d u c t - f o r m , i
. e . µ ( 2 ) = µ ⊗ µ ( w h i c h i s e q u i v a l e n t t o e
[ h z , φ 1 i · h z , φ 2 i ] = h µ , φ 1 i · h µ , φ
2 i ( t h i s i s i n f a c t e q u i v a l e n t t o e [ h z , φ i 2
]

Figure 3: Example of two of the segments from a
document in the arXiv test set.

Alg
WD
Pk
S
oC99 G 47.29∗ 47.31∗
oC99 R 47.60∗ 49.30∗
28.23
CVS G 26.07
27.73
CVS R 25.55
CVSn G 24.63
26.69
CVSn R 24..03 26..15

Table 5:
Results on the arXiv test set for the
C99 method using word vectors (oC99), our CVS
method, and CVS method with normalized word
vectors (CVSn). The Pk and WD metrics are given
for both the greedy (G) and reﬁned splitting strate-
gies (R), with respect to the reference segmentation
in the test set. The reﬁned strategy was allowed up
to 20 iterations to converge. The reﬁnement con-
verged for all of the CVS runs, but failed to converge
for some documents in the test set under the C99
method. Reﬁnement improved performance in all
cases, and our CVS methods improve signiﬁcantly
over the C99 method for this task. (*) The oC99 re-
sults are computed on the ﬁrst 100 test documents
due to lack of time. The results should still be rep-
resentative.

We see that the CVS method performs far better on the
test set than the C99 style segmentation using word vectors.
The Pk and WD values obtained are not as impressive as
those obtained on the Choi test set, but this test set oﬀers
a much more challenging segmentation task: it requires the
methods to work at the level of words, and as well includes
the possibility that natural topic boundaries occur in the test
set segments themselves. The segmentations obtained with
the CVS method typically appear sensibly split on section
boundaries, references and similar formatting boundaries,
not known in advance to the algorithm.
As a ﬁnal illustration of the eﬀectiveness of our algorithm
at segmenting scientiﬁc articles, we’ve applied the best per-
forming algorithm to the current article. Figure 4 shows
how the algorithm segments the article roughly along sec-
tion borders.

5. CONCLUSION
We have presented a general framework for describing and
developing segmentation algorithms, and compared some ex-
isting and new strategies for representation, scoring and
splitting. We have demonstrated the utility of semantic
word embeddings for segmentation, both in existing algo-
rithms and in new segmentation algorithms. On a real world
segmentation task at word level, we’ve demonstrated the
ability to generate useful segmentations of scientiﬁc articles.
In future work, we plan to use this segmentation technique
to facilitate retrieval of documents with segments of con-
centrated content, and to identify documents with localized
sections of similar content.

6. ACKNOWLEDGEMENTS
This work was supported by NSF IIS-1247696. We thank
James P. Sethna for useful discussions and for feedback on
the manuscript.

7. REFERENCES
[1] S. Arora, Y. Li, T. M. Yingyu Liang, and A. Risteski.
Random walks on context spaces: Towards an
explanation of the mysteries of semantic word
embeddings. 2015, arXiv:1502.03520.
[2] D. Beeferman, A. Berger, and J. Laﬀerty. Statistical
models for text segmentation. Machine learning,
34(1-3):177–210, 1999.
[3] F. Y. Choi. Advances in domain independent linear
text segmentation. In Proceedings of the 1st North
American chapter of the Association for
Computational Linguistics conference, pages 26–33.
Association for Computational Linguistics, 2000,
arXiv:cs/0003083.
[4] F. Y. Choi, P. Wiemer-Hastings, and J. Moore. Latent
semantic analysis for text segmentation. In In
Proceedings of EMNLP. Citeseer, 2001.
[5] A. Coates and A. Y. Ng. Learning feature
representations with k-means. In Neural Networks:
Tricks of the Trade, pages 561–580. Springer, 2012.
[6] S. T. Dumais. Latent semantic analysis. Annual review
of information science and technology, 38(1):188–230,
2004.
[7] P. Fragkou, V. Petridis, and A. Kehagias. A dynamic
programming algorithm for linear text segmentation.

Journal of Intel ligent Information Systems,
23(2):179–197, 2004.
[8] M. A. Hearst. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
linguistics, 23(1):33–64, 1997.
[9] O. Levy and Y. Goldberg. Neural word embedding as
implicit matrix factorization. In Advances in Neural
Information Processing Systems, pages 2177–2185,
2014.
[10] T. Mikolov, Q. V. Le, and I. Sutskever. Exploiting
similarities among languages for machine translation.
arXiv preprint arXiv:1309.4168, 2013.
[11] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages
3111–3119, 2013.
[12] H. Misra, F. Yvon, J. M. Jose, and O. Cappe. Text
segmentation via topic modeling: an analytical study.
In Proceedings of the 18th ACM conference on
Information and know ledge management, pages
1553–1556. ACM, 2009.
[13] J. Pennington, R. Socher, and C. D. Manning. Glove:
Global vectors for word representation. Proceedings of
the Empiricial Methods in Natural Language
Processing (EMNLP 2014), 12, 2014.
[14] L. Pevzner and M. A. Hearst. A critique and
improvement of an evaluation metric for text
segmentation. Computational Linguistics, 28(1):19–36,
2002.
[15] M. Riedl and C. Biemann. Text segmentation with
topic models. Journal for Language Technology and
Computational Linguistics, 27(1):47–69, 2012.
[16] E. Terzi et al. Problems and algorithms for sequence
segmentations. 2006.
[17] M. Utiyama and H. Isahara. A statistical model for
domain-independent text segmentation. In Proceedings
of the 39th Annual Meeting on Association for
Computational Linguistics, pages 499–506. Association
for Computational Linguistics, 2001.

Figure 4: Eﬀect of applying our segmentation algo-
rithm to this paper.

APPENDIX
A. DETAILS OF C99 REPRESENTATION
This set of experiments compare to the results reported in
[3]. We implemented our own version of the C99 algorithm
(oC99) and tested it on the Choi dataset. We explored the
eﬀect of various changes to the representation part of the
algorithm, namely the eﬀects of removing stop words, cut-
ting small sentence sizes, stemming the words, and perform-
ing the rank transformation on the cosine similarity matrix.
For stemming, the implementation of the Porter stemming
algorithm from nltk was used. For stopwords, we used the
list distributed with the C99 code augmented by a list of
punctuation marks. The results are summarized in Table 6.
While we reproduce the results reported in [4] without
the rank transformation (C99 in table 6), our results for
the rank transformed results (last two lines for oC99) show
better performance without stemming. This is likely due to
particulars relating to details of the text transformations,
such at the precise stemming algorithm and the stopword
list. We attempted to match the choices made in [4] as
much as possible, but still showed some deviations.
Perhaps the most telling deviation is the 1.5% swing in
results for the last two rows, whose only diﬀerence was a
change in the tie breaking behavior of the algorithm. In our
best result, we minimized the ob jective at each stage, so in
the case of ties would break at the earlier place in the text,
whereas for the TBR row, we maximized the negative of the
ob jective, so in the case of ties would break on the rightmost
equal value.
These relatively large swings in the performance on the
Choi dataset suggest that it is most appropriate to com-
pare diﬀerences in parameter settings for a particular im-
plementation of an algorithm. Comparing results between
diﬀerent articles to assess performance improvements due to
algorithmic changes hence requires careful attention to the
implemention details.

note
C99 [4]

oC99

Reps

TBR
Best

cut
0
0
0
0
0
0
0
5
0
0
0
5
5
5
5
5
5
5
5
5

stop
T
T
T
T
T
T
F
F
T
F
T
T
T
T
T
T
T
T
T
T

stem rank
F
0
11(?)
F
11(?)
T
0
F
11
F
11
T
F
0
0
F
0
F
0
T
0
T
T
0
3
T
5
T
7
T
T
9
11
T
13
T
11
F
F
11

Pk (%) WD (%)
-
23
13
-
-
12
22.52
22.52
16.72
16.69
19.96
17.90
32.26
32.28
32.76
32.73
22.52
22.52
32.28
32.26
23.33
23.33
23.56
23.59
18.30
18.17
17.56
17.44
17.05
16.95
17.12
17.20
17.14
17.07
17.19
17.11
17.12
17.04
15.56
15.64

Table 6: Eﬀects of text representation on the per-
formance of the C99 algorithm. The cut column de-
notes the cutoﬀ for the length of a sentence after pre-
processing. The stop column denotes whether stop
words and punctuation are removed. The stem col-
umn denotes whether the words are passed through
the Porter stemming algorithm. The rank column
denotes the size of the kernel for the ranking trans-
formation. Evaluations are given both as the Pk
metric and the Window Diﬀ (WD) score. All ex-
periments are done on the 400 test documents in
the 3–11 set of the Choi dataset. The upper sec-
tion cites results contained in the CWM 2000 paper
[4]. The second section is an attempt to match these
results with our implementation (oC99). The third
section attempts to give an overview of the eﬀect of
diﬀerent parameter choices for the representation
step of the algorithm. The last section reports our
best observed result as well as a run (TBR) with
the same parameter settings, but with a tie-breaking
strategy that takes right-most rather then left-most
equal value.

